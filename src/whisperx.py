# -*- coding: utf-8 -*-
"""WhisperX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YXNpwCusWWggpMZwPlnuuSAfA-stMXFr

# Machine Transcription

Timer
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
# %load_ext autotime

"""## Step 1: Install

Execute all steps.
"""

#@title Step 1.1: GPU Model
!nvidia-smi

"""You need a GPU with minimal 11GiB VRAM: if not, turn it on under "Runtime - Change Runtime Type"."""

# Commented out IPython magic to ensure Python compatibility.
#@title Step 1.2 Install packages - takes ~2 mins
# %pip install srt requests tqdm httpx aiometer --quiet
# !wget -q https://chineseaci.com/tools/megadl ./megadl
!chmod +x ./megadl
!pip uninstall torch torchvision torchaudio -y
!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121
!pip install ctranslate2==4.4.0
!pip install faster-whisper==1.1.0
!pip install whisperx==3.3.1
!pip install ffmpeg-python
!apt-get install libcudnn8=8.9.2.26-1+cuda12.1
!apt-get install libcudnn8-dev=8.9.2.26-1+cuda12.1

# https://github.com/m-bain/whisperX/issues/118
!pip install pyannote-audio -U --quiet

import locale
locale.getpreferredencoding = lambda: "UTF-8"

"""## Step 2: Setup WhisperX

WhisperX has 2 ways for transcription:

1. Run the whole file in Whisper, like the original repo;
2. Run Voice Activity Detection(VAD), and only run Whisper for sections with human voice.

We use approach 2 by default.

### Variables

See [original repo](https://github.com/m-bain/whisperX/blob/main/whisperx/transcribe.py#L574-L616) for uncovered variables. They are setup as default value.

- `vad_filter`: Run Voice Activity Detection before Whisper Sound To Text - author (reported improved performance)[https://arxiv.org/abs/2303.00747].
- `hf_token`: If VAD or diarization(not used in this notebook) is needed a valid Huggingface token on an account that has accepted EULA for models is required: Follow guidance at [original repo](https://github.com/m-bain/whisperX#voice-activity-detection-filtering--diarization) and fill in `hf_token` with generated Huggingface API key.
- `parallel_bs`: Number of Whisper tasks to execute in parallel. Only valid if `vad_filter` is set to True. VAD shall cut audio input into multiple smaller segments - this variable controls number of segments to run at the same time.
- `transcription_cutoff_char`: Max number of chars per line.
- `transcription_sentence_interval`: We try to merge short sentences into longer ones if they are close enough to avoid having subtitle flashing on screen for too short - but shall separate sentences if no voice activity could be detected for too long. This setup the max interval for consideration.
- `translation_thread`: Number of requests to send out at the same time.
- `translation_lines_per_request`: The author shall send multiple lines to translation engine in the same API call to provide some context for increased quality - but some engines may have much lower input length limit.
"""

#@title Step 2.1: Import model and setup parameters
# setup model
import torch

vad_filter = False #@param {type:"boolean"}
hf_compute_type = 'float16'
align_extend = 2 #@param {type:"integer"}
align_from_prev = True #@param {type:"boolean"}
batch_size = 32
transcription_cutoff_char = 120 #@param {type:"integer"}
transcription_sentence_interval = 1.5 #@param {type:"number"}

# device = "cuda"
device = "cuda" if torch.cuda.is_available() else "cpu"

import whisperx

#@title Step 2.2: Select and load Model.
# Whisper is on large-v2 by default
model_name = 'large-v2' #@param ["tiny", "small", "medium", "large-v2", "tiny.en", "small.en", "medium.en", "large-v3"]

model = whisperx.load_model(model_name, device=device, compute_type=hf_compute_type)

"""Colab should have enough VRAM for any model selected on *any* GPU provided, including `large` - expect ~13G of VRAM usage for `large` model. Prefer `medium` than `medium.en` per [original paper](https://cdn.openai.com/papers/whisper.pdf).

Disconnect and reconnect if you change the model in the middle of execution(and only do so if you know what you are doing) to avoid VRAM OOM. All progress and uploaded/generated files shall be lost.

Select desired model and run the cell above. It takes ~3 mins to download the `medium` model: move to Step 3 while waiting.

## Step 3: Prepare audio for transcription

### Step 3.1: Convert video to audio

Uploading a file then sizing it to fit 16000Hz rate of
"""

import ffmpeg
from google.colab import files

uploaded = files.upload()
uploaded_file_name = list(uploaded.keys())[0]

input_video_file = uploaded_file_name
output_audio_file = "aci.wav"

print(f"Starting audio extraction from '{input_video_file}' to '{output_audio_file}'...")
try:

    (
        ffmpeg
        .input(input_video_file)
        .output(
            output_audio_file,
            vn=None,
            ar=16000,
            acodec='pcm_s16le'
        )
        .run(capture_stdout=True, capture_stderr=True)
        # .overwrite_output()
    )

    print("FFmpeg command executed successfully!")
    print(f"Audio extraction complete. Output saved as '{output_audio_file}'")

except ffmpeg.Error as e:
    print("Error during FFmpeg execution:", file=sys.stderr)
    print("FFmpeg stderr output:", file=sys.stderr)
    try:
        print(e.stderr.decode(), file=sys.stderr)
    except Exception as decode_err:
         print(f"Could not decode stderr: {decode_err}", file=sys.stderr)

# !ffmpeg -i "AtomicHabitsAnEasyProvenWaytoBuildGoodHabitsBreakBadOnes_ep6.flac" -vn -ar 16000 -c:a pcm_s16le aci.wav
# !ffmpeg -i "AIDS, COVID-19 and Art- Reflections on Pandemics.mp4"  -vn -ar 16000 -c:a pcm_s16le aci.wav

"""### Step 3.2: Upload audio file to Colab

Rename the file to something benign - without space or any special character.

Click the "file" icon on the left, click the "upload to session storage" button to upload the audio file. Maximum upload speed is ~1.1MB/s.

Input the exact name of your uploaded file to the field below. Select the main language of the audio.

**NOTE: All uploaded and generated files are strictly for this session and shall be deleted when you disconnect from the instance - no recovery possible!**

Type in the name of the audio file you uploaded below, should be ending with `.wav` and select language of your audio file.
"""

audio_file_name = 'aci.wav' #@param {type:"string"}
audio_file_language = 'en' #@param ['en', 'cn']

"""## Step 4: Transcribe

Execute the steps below.

Expected speed for `medium.en` model is ~5X on T4 - aka 45 min episode should take ~8 mins. Larger model shall take longer to process.

Wait till the process is finished.

If you have enough VRAM you can keep Whisper model in VRAM; or, if you have enough RAM, use `model = model.cpu()` to offload to CPU in case you want to come back again.
"""

#@title Step 4.1 Transcribe the audio file
# Do the work
import torch
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False
audio_path = audio_file_name

print("Performing transcription...")
audio = whisperx.load_audio(audio_file_name)
result = model.transcribe(audio, batch_size=batch_size, language=audio_file_language, verbose=True, print_progress=True)
print(result["segments"])

#@title Step 4.2 (Optional) Offload Whisper model to free up GPU memory
# offload the model to free up GPU memory - we only got 16G VRAM on Colab with T4 but should be enough for medium model and VAD
# model = model.cpu()
del model

"""## Step 5: Forced Alignment

With transcription and _better_ timestamp we now use forced alignment to acquire per-word(and per-char) timestamp and redo sentence segmentation manually.

Execute the steps below.
"""

#@title Step 5.1 Load the alignment model
model_alignment, metadata_alignment = whisperx.load_align_model(language_code=result['language'], device=device)

#@title Step 5.2 Conduct forced alignment

result_aligned = whisperx.alignment.align(result["segments"], model_alignment, metadata_alignment, audio, device)
print(result["segments"])

#@title Step 5.3 Setup timecode reformatting

import copy

def word_segment_to_sentence(segments, max_text_len=80):
    """
    Convert word segments to sentences.
    :param segments: [{"text": "Hello,", "start": 1.1, "end": 2.2}, {"text": "World!", "start": 3.3, "end": 4.4}]
    :type segments: list of dicts
    :return: Segments, but with sentences instead of words.
    :rtype: list of dicts  [{"text": "Hello, World!", "start": 1.1, "end": 4.4}]
    """
    end_of_sentence_symbols = tuple(['.', '!', '?', ',', ';', ':'])
    sentence_results = []

    current_sentence = {"text": "", "start": 0, "end": 0}
    current_sentence_template = {"text": "", "start": 0, "end": 0}

    for segment in segments:
        if current_sentence["text"] == "":
            current_sentence["start"] = segment["start"]
        current_sentence["text"] += ' ' + segment["text"] + ' '
        current_sentence["end"] = segment["end"]
        if segment["text"][-1] in end_of_sentence_symbols:
            current_sentence["text"] = current_sentence["text"].strip()
            sentence_results.append(copy.deepcopy(current_sentence))
            current_sentence = copy.deepcopy(current_sentence_template)
    return sentence_results



def sentence_segments_merger(segments, max_text_len=80, max_segment_interval=2):
    """
    Merge sentence segments to one segment, if the length of the text is less than max_text_len.
    :param segments: [{"text": "Hello, World!", "start": 1.1, "end": 4.4}, {"text": "Hello, World!", "start": 1.1, "end": 4.4}]
    :type segments: list of dicts
    :param max_text_len: Max length of the text
    :type max_text_len: int
    :return: Segments, but with merged sentences.
    :rtype: list of dicts  [{"text": "Hello, World! Hello, World!", "start": 1.1, "end": 4.4}]
    """

    merged_segments = []
    current_segment = {"text": "", "start": 0, "end": 0}
    current_segment_template = {"text": "", "start": 0, "end": 0}

    for segment in segments:
        if current_segment["text"] == "":
            current_segment["start"] = segment["start"]


        if segment["start"] - current_segment["end"] < max_segment_interval and \
                len(current_segment["text"] + " " + segment['text']) < max_text_len:
            # print('merge')
            current_segment["text"] += ' ' + segment["text"]
            current_segment["end"] = segment["end"]
        else:
            current_segment["text"] = current_segment["text"].strip()
            merged_segments.append(copy.deepcopy(current_segment))
            current_segment = copy.deepcopy(segment)

    return merged_segments



"""## Step 6: Collect results

### Step 6.1: Convert transcription to SRT

Execute the 3 cells below to peek the result.
"""

#@title Import packages
import srt
from datetime import timedelta

#@title Create SRT with transcription
result_srt_list = []
result_merged = sentence_segments_merger(result_aligned['segments'], max_text_len=transcription_cutoff_char, max_segment_interval=transcription_sentence_interval)

for i, v in enumerate(result_merged):
    result_srt_list.append(srt.Subtitle(index=i, start=timedelta(seconds=v['start']), end=timedelta(seconds=v['end']), content=v['text'].strip()))

composed_transcription = srt.compose(result_srt_list)

#@title Optional: Peek the transcription SRT file
print(composed_transcription)

"""### Step 6.2: Generate and download transcribed srt

Input desired name of the file for transcribed srt below, and execute the 2 cells below.
"""

#@title Step 6.2 Name of the transcribed srt to generate, should be ending with `.srt`

transcribed_srt_name = 'transcribed.srt' #@param {type:"string"}

#@title Write the SRT
with open(transcribed_srt_name, 'w') as f:
    f.write(composed_transcription)

"""You should see a `srt` file generated with desired name: right click and download the file."""