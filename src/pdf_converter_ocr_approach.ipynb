{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encompasses the approach to text processing with a more OCR approach.\n",
    "1. Gray-scale ->  Blur image -> draw ROI of blurred dark area -> Retrieve ROI as Rect -> Extract text using Rect with PyMuPDF\n",
    "2. Possible drawback:\n",
    "- Inability to detect text in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "class MedianFinder(object):\n",
    "    def __init__(self):\n",
    "        self.minheap = []\n",
    "        self.maxheap = []\n",
    "\n",
    "    def addNum(self, num:float):\n",
    "        if not self.minheap and not self.maxheap:\n",
    "            heapq.heappush(self.maxheap, -num)\n",
    "        else:\n",
    "            if len(self.minheap)== len(self.maxheap):\n",
    "                if -self.maxheap[0]<=num:\n",
    "                    heapq.heappush(self.minheap, num)\n",
    "                    heapq.heappush(self.maxheap, -heapq.heappop(self.minheap))\n",
    "                else:\n",
    "                    heapq.heappush(self.maxheap, -num)\n",
    "            else:\n",
    "                if -self.maxheap[0]<= num:\n",
    "                    heapq.heappush(self.minheap, num)\n",
    "                else:\n",
    "                    heapq.heappush(self.minheap, -heapq.heappop(self.maxheap))\n",
    "                    heapq.heappush(self.maxheap, -num)\n",
    "                \n",
    "    def findMedian(self):\n",
    "        if len(self.minheap)==len(self.maxheap):\n",
    "            return (self.minheap[0]-self.maxheap[0])/2.0\n",
    "        else:\n",
    "            return -self.maxheap[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, content):\n",
    "        self.content = content\n",
    "        self.children = []\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "    def __str__(self):\n",
    "        return f\"Node({self.content}, {self.children})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def should_merge(rect1, rect2, h_threshold=4, v_threshold=1) -> Tuple[str, bool]:\n",
    "    \"\"\"\n",
    "    Determine if two rectangles should be merged based on proximity and alignment.\n",
    "    \n",
    "    Args:\n",
    "        rect1, rect2: PyMuPDF Rect objects\n",
    "        h_threshold: Maximum horizontal gap between regions to consider merging\n",
    "        v_threshold: Maximum vertical gap between regions to consider merging\n",
    "        line_height_factor: Factor to determine if rects are on same line\n",
    "        \n",
    "    Returns:\n",
    "        str: Type of merge ('horizontal', 'vertical', 'none')\n",
    "        bool: True if rectangles should be merged\n",
    "    \"\"\"\n",
    "    # Check if rectangles are on approximately the same line\n",
    "    height1 = rect1.height\n",
    "    height2 = rect2.height\n",
    "\n",
    "    avg_height = (height1 + height2) / 2\n",
    "    \n",
    "    same_line = abs(rect1.y0 - rect2.y0) < avg_height * 0.75 \n",
    "    # Check horizontal proximity\n",
    "    if rect1.x1 < rect2.x0: \n",
    "        h_gap = rect2.x0 - rect1.x1\n",
    "        return same_line and h_gap < h_threshold\n",
    "    elif rect2.x1 < rect1.x0:  \n",
    "        h_gap = rect1.x0 - rect2.x1\n",
    "        return same_line and h_gap < h_threshold\n",
    "    \n",
    "    # Merging Intervals (LC)\n",
    "    # Check if they completely overlap horizontally\n",
    "    overlap_x = (rect1.x0 > rect2.x0 and rect2.x1 > rect1.x1) \\\n",
    "                or (rect2.x0 > rect1.x0 and rect1.x1 > rect2.x1)\n",
    "    # if it overlaps horizontally by over 75% of the smaller rect, then consider it overlapping\n",
    "    overlap_x = overlap_x or (max(rect1.x0, rect2.x0) < min(rect1.x1, rect2.x1) \\\n",
    "                              and max(rect1.x0,rect2.x0) - min(rect1.x1,rect2.x1) > min(rect1.width,rect2.width)*0.75)\n",
    "    if overlap_x:\n",
    "        if rect1.y1 < rect2.y0:\n",
    "            v_gap = rect2.y0 - rect1.y1\n",
    "            return v_gap < v_threshold\n",
    "        elif rect2.y1 < rect1.y0:  \n",
    "            v_gap = rect1.y0 - rect2.y1\n",
    "            return v_gap < v_threshold \n",
    "    \n",
    "    return overlap_x and (max(rect1.y0, rect2.y0) < min(rect1.y1, rect2.y1))\n",
    "\n",
    "def merge_text_regions(regions, iterations=3):\n",
    "    \"\"\"\n",
    "    Merge text regions that are close to each other.\n",
    "    \n",
    "    Args:\n",
    "        regions: List of PyMuPDF Rect objects\n",
    "        iterations: Number of merging passes to perform\n",
    "    Returns:\n",
    "        list: Merged PyMuPDF Rect objects\n",
    "    \"\"\"\n",
    "    if not regions:\n",
    "        return []\n",
    "    \n",
    "    # Perform multiple iterations of merging to handle chains of regions\n",
    "    for _ in range(iterations):\n",
    "        merged = False\n",
    "        i = 0\n",
    "        while i < len(regions):\n",
    "            j = i + 1\n",
    "            while j < len(regions):\n",
    "                if should_merge(regions[i], regions[j], h_threshold=6, v_threshold=0.5): # Make this more relative\n",
    "                    # Merge rectangles\n",
    "                    merged_rect = pymupdf.Rect(\n",
    "                        min(regions[i].x0, regions[j].x0),\n",
    "                        min(regions[i].y0, regions[j].y0),\n",
    "                        max(regions[i].x1, regions[j].x1),\n",
    "                        max(regions[i].y1, regions[j].y1)\n",
    "                    )\n",
    "                    regions[i] = merged_rect\n",
    "                    regions.pop(j)\n",
    "                    merged = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "            \n",
    "        if not merged:\n",
    "            break\n",
    "            \n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_ocr_approach(pdf_path, page_num=0, visual_proof = False):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF using image processing to identify text regions.\n",
    "    With text region merging to combine related text blocks.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        page_num (int): Page number to process (0-indexed)\n",
    "        \n",
    "    Returns:\n",
    "        list: Extracted text segments from identified regions\n",
    "    \"\"\"\n",
    "    # Open the PDF\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    page = doc[page_num]\n",
    "    zoom_factor = 2\n",
    "    # Convert page to image\n",
    "    pix = page.get_pixmap(matrix=pymupdf.Matrix(zoom_factor, zoom_factor))\n",
    "    img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    if img.shape[2] >= 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = img[:, :, 0]\n",
    "    # Apply Gaussian blur\n",
    "    adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 13, 2)\n",
    "    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 9))\n",
    "    \n",
    "    dilated = cv2.dilate(adaptive, rectKernel, iterations=1)\n",
    "    \n",
    "    closing = cv2.morphologyEx(dilated, cv2.MORPH_CLOSE, rectKernel)\n",
    "    \n",
    "    edged = cv2.Canny(closing, 50, 200, apertureSize=3)\n",
    "\n",
    "    contours, _  = cv2.findContours(\n",
    "        edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    \n",
    "    # Create ROIs from contours\n",
    "    initial_regions = []\n",
    "    for cnt in contours:\n",
    "        \n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        # Convert back to PDF coordinates (accounting for the zoom factor)\n",
    "        pdf_x, pdf_y = x/zoom_factor, y/zoom_factor\n",
    "        pdf_w, pdf_h = w/zoom_factor, h/zoom_factor\n",
    "        \n",
    "        # Create PyMuPDF rect\n",
    "        rect = pymupdf.Rect(pdf_x, pdf_y, pdf_x + pdf_w, pdf_y + pdf_h)\n",
    "        initial_regions.append(rect)\n",
    "    \n",
    "    # Merge text regions that are close to each other\n",
    "    merged_regions = merge_text_regions(initial_regions, iterations=5)\n",
    "    \n",
    "    # Extract text from merged regions\n",
    "    text_regions = []\n",
    "    text_height_collection = MedianFinder()\n",
    "    \n",
    "    words = page.get_text(\"words\")\n",
    "\n",
    "    for rect in sorted(merged_regions, key=lambda r: [(r.y1- r.y0) * (r.x1 -r.x0), r.y0, r.x0]):\n",
    "        text = \"\"\n",
    "        idx = 0\n",
    "        min_text_height = 1000000\n",
    "        # Add relative padding to the rect to ensure we catch words that are nearby\n",
    "        padding = 2  # pixels in PDF coordinate space\n",
    "        rect = pymupdf.Rect(\n",
    "            rect.x0 - padding,\n",
    "            rect.y0 - padding,\n",
    "            rect.x1 + padding,\n",
    "            rect.y1 + padding\n",
    "        )\n",
    "        # Iterate through words and pick out words that fall within the rect\n",
    "        prev_y = words[0][1] if words else 0\n",
    "        while words and idx < len(words):\n",
    "            current_word = words[idx]\n",
    "            word_rect = pymupdf.Rect(current_word[:4])\n",
    "            \n",
    "            if word_rect.intersects(rect) or rect.contains(word_rect):        \n",
    "                text_height_collection.addNum(word_rect.height)\n",
    "                min_text_height = min(min_text_height, word_rect.height) \n",
    "                text += current_word[4] + (\" \" if abs(prev_y - current_word[1]) <= abs(current_word[1] - current_word[3])*0.5 else \"\\n\")\n",
    "                prev_y = current_word[1]\n",
    "                words.pop(idx)\n",
    "            else:\n",
    "                idx += 1\n",
    "        \n",
    "        if text.strip():\n",
    "            if text.strip().isdigit():\n",
    "                continue\n",
    "            text_regions.append({\"occupy_space\": rect,\n",
    "                                 \"content\": text, \n",
    "                                 \"text_height_median\": text_height_collection.findMedian(), \n",
    "                                 \"min_height\": min_text_height, \n",
    "                                 \"page_num\": page_num})\n",
    "    # print(words)\n",
    "    if visual_proof:\n",
    "        viz_img = img.copy()\n",
    "        for item in text_regions:\n",
    "            rect = item[\"occupy_space\"]\n",
    "            # Convert back to image coordinates\n",
    "            x0, y0, x1, y1 = rect.x0*zoom_factor, rect.y0*zoom_factor, rect.x1*zoom_factor, rect.y1*zoom_factor\n",
    "            cv2.rectangle(viz_img, (int(x0), int(y0)), (int(x1), int(y1)), (0, 255, 0), 2)\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(edged, cmap='gray')\n",
    "        plt.title(\"Processed Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(viz_img)\n",
    "        plt.title(f\"Merged Text Regions: {len(text_regions)} blocks\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return text_regions[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1/4...\n",
      "Processing page 2/4...\n",
      "Processing page 3/4...\n",
      "Processing page 4/4...\n",
      "[0, 0, 11, 0, 0, 0, 7, 0, 2, 1, 0, 2, 1, 0]\n",
      "CPU times: total: 984 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Example usage\n",
    "input_pdf_path = r\"D:\\DATA300\\AudioBookSum\\pdf\\Clopath.pdf\"\n",
    "doc = pymupdf.open(input_pdf_path)\n",
    "text = \"\"\n",
    "median_collection = []\n",
    "content = []\n",
    "i= 1\n",
    "possible_water_mark = None\n",
    "has_wm = False\n",
    "for i in range(len(doc[:5])):\n",
    "    extracted_pages = sorted(extract_text_with_ocr_approach(input_pdf_path, page_num =i, visual_proof=False), key = lambda x: [x[\"occupy_space\"].y0, x[\"occupy_space\"].x0])\n",
    "    # save all reapeating elements\n",
    "    if possible_water_mark and extracted_pages[-1][\"content\"] == possible_water_mark:\n",
    "        has_wm = True\n",
    "    if not extracted_pages:\n",
    "        continue\n",
    "\n",
    "\n",
    "    possible_water_mark = extracted_pages[-1][\"content\"]\n",
    "\n",
    "\n",
    "for i in range(len(doc)):\n",
    "    print(f\"Processing page {i+1}/{len(doc)}...\")\n",
    "    extracted_pages = sorted(extract_text_with_ocr_approach(input_pdf_path, page_num =i, visual_proof=False), key = lambda x: [x[\"occupy_space\"].y0, x[\"occupy_space\"].x0])\n",
    "    if has_wm:\n",
    "        extracted_pages.pop(0)\n",
    "        extracted_pages.pop()\n",
    "    \n",
    "    content.extend(extracted_pages)\n",
    "\n",
    "# Classify the text regions based on their height and position\n",
    "content.sort(key =lambda x: ( x[\"page_num\"], x[\"occupy_space\"].y0, x[\"occupy_space\"].x0))\n",
    "# Rank the text regions based on their height and position\n",
    "base = \"\"\n",
    "curr_rank = 1\n",
    "count_till_match_size = [0] * len(content)\n",
    "stack = []\n",
    "for idx, item in enumerate(content):\n",
    "    while stack and item[\"text_height_median\"] >= content[stack[-1]][\"text_height_median\"]: # padding:\n",
    "        i = stack.pop()\n",
    "        count_till_match_size[i] = idx - i - 1\n",
    "    stack.append(idx)\n",
    "while stack:\n",
    "    i = stack.pop()\n",
    "    count_till_match_size[i] = len(content) - i - 1\n",
    "\n",
    "node_list = []\n",
    "# Graph DFS traversal\n",
    "for i in content:\n",
    "    curr = Node(i)\n",
    "    node_list.append(curr)\n",
    "    # print(str(curr))\n",
    "\n",
    "\n",
    "for i in range(len(node_list)):\n",
    "    if count_till_match_size[i] > 0:\n",
    "        for j in range(count_till_match_size[i]):\n",
    "            node_list[i].add_child(node_list[(i + 1) + j])\n",
    "    visited = set()\n",
    "\n",
    "chapters = [i for i in node_list if len(i.children) > 0]\n",
    "print(count_till_match_size)\n",
    "if not chapters or sum(count_till_match_size) == 0:\n",
    "    chapters = node_list\n",
    "    \n",
    "for chapter in chapters:\n",
    "    if chapter not in visited:\n",
    "        # text += str(chapter.content[\"text_height_median\"]) +\",\" \n",
    "        text +=chapter.content[\"content\"] + \"\\n\"\n",
    "        visited.add(chapter)\n",
    "        if chapter.content[\"content\"].strip() in [\"Notes\", \"Notes and References\", \"Note\"]:\n",
    "\n",
    "            continue\n",
    "        for child in chapter.children:\n",
    "            if child not in visited:\n",
    "                # text += \"\\t\" + str(child.content[\"min_height\"]) +\",\" \n",
    "                text += \"\\t\" + child.content[\"content\"] + \"\\n\"\n",
    "                visited.add(child)\n",
    "# print(text)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_text(text_blocks, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Chunk text blocks into smaller pieces while preserving page and bounding box information.\n",
    "    \n",
    "    Args:\n",
    "        text_blocks (list): List of dictionaries containing text with metadata (page, bbox, etc.)\n",
    "        chunk_size (int): Maximum size of each chunk in characters\n",
    "        chunk_overlap (int): Number of overlapping characters between chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing chunks with preserved metadata\n",
    "    \"\"\"\n",
    "    chunked_data = []  # Initialize as a list, not a dictionary\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # First combine all text blocks into a single document string for chunking\n",
    "    combined_text = \"\"\n",
    "    metadata_map = {}  # Map character positions to original metadata\n",
    "    current_position = 0\n",
    "    \n",
    "    # Process each text block and map character positions to metadata\n",
    "    for idx, block in enumerate(text_blocks):\n",
    "        # Get text and metadata\n",
    "        text = block.get(\"content\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        block_length = len(text)\n",
    "        page = block.get(\"page_num\", 0)\n",
    "        bbox = block.get(\"occupy_space\", None)\n",
    "        text_height = block.get(\"text_height_median\", None)\n",
    "        \n",
    "        # Map each character position in the combined text to its source metadata\n",
    "        for i in range(current_position, current_position + block_length):\n",
    "            metadata_map[i] = {\n",
    "                \"page\": page,\n",
    "                \"bbox\": bbox,\n",
    "                \"text_height\": text_height,\n",
    "                \"original_block_index\": idx  # Use idx instead of text_blocks.index()\n",
    "            }\n",
    "        \n",
    "        # Add text to combined string with a marker between blocks\n",
    "        combined_text += text + \" [BLOCK_BREAK] \"\n",
    "        current_position += block_length + len(\" [BLOCK_BREAK] \")\n",
    "    \n",
    "    # Split the combined text into chunks\n",
    "    chunks = text_splitter.split_text(combined_text)\n",
    "    \n",
    "    # For each chunk, determine the predominant page and bounding box\n",
    "    chunk_start = 0\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Find the start position of this chunk in the combined text\n",
    "        if i == 0:\n",
    "            start_pos = 0\n",
    "        else:\n",
    "            # Look for chunk in the vicinity of where we expect it\n",
    "            search_start = max(0, chunk_start - chunk_overlap)\n",
    "            search_text = combined_text[search_start:search_start + len(chunk) + 100]\n",
    "            rel_pos = search_text.find(chunk[:50])  # Use prefix to locate chunk\n",
    "            if rel_pos != -1:\n",
    "                start_pos = search_start + rel_pos\n",
    "            else:\n",
    "                # Fallback if exact match not found\n",
    "                start_pos = chunk_start\n",
    "        \n",
    "        # Update for next iteration\n",
    "        chunk_start = start_pos + len(chunk) - chunk_overlap\n",
    "            \n",
    "        # Find the most common page and bounding box in this chunk\n",
    "        page_counts = {}\n",
    "        bbox_by_page = {}\n",
    "        \n",
    "        for j in range(start_pos, min(start_pos + len(chunk), len(combined_text))):\n",
    "            if j in metadata_map:\n",
    "                page = metadata_map[j][\"page\"]\n",
    "                if page not in page_counts:\n",
    "                    page_counts[page] = 0\n",
    "                    bbox_by_page[page] = []\n",
    "                    \n",
    "                page_counts[page] += 1\n",
    "                if metadata_map[j][\"bbox\"] and metadata_map[j][\"bbox\"] not in bbox_by_page[page]:\n",
    "                    bbox_by_page[page].append(metadata_map[j][\"bbox\"])\n",
    "        \n",
    "        # Find predominant page\n",
    "        predominant_page = max(page_counts.items(), key=lambda x: x[1])[0] if page_counts else 0\n",
    "        \n",
    "        # Create enhanced chunk with metadata\n",
    "        enhanced_chunk = {\n",
    "            \"text\": chunk.replace(\" [BLOCK_BREAK] \", \"\\n\\n\"),\n",
    "            \"page\": predominant_page,\n",
    "            \"bbox_list\": bbox_by_page.get(predominant_page, []),\n",
    "            \"start_position\": start_pos,\n",
    "            \"chunk_index\": i\n",
    "        }\n",
    "        \n",
    "        chunked_data.append(enhanced_chunk)\n",
    "    \n",
    "    return chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_interesting_points(chunked_data, llm, text_blocks, file_name, threshold=5):\n",
    "    \"\"\"\n",
    "    Identify important points in text using LLM, adapted to work with chunk_text output.\n",
    "    \n",
    "    Args:\n",
    "        chunked_data (list): List of dictionaries returned by chunk_text function\n",
    "        llm: LLM model for generating interesting points\n",
    "        text_blocks (list): Original text blocks with metadata\n",
    "        file_name (str): Path to cache file\n",
    "        threshold (int): Minimum word count for segments to be considered\n",
    "        \n",
    "    Returns:\n",
    "        list: List of interesting sections with metadata and page annotations\n",
    "    \"\"\"\n",
    "    interesting_sections = []\n",
    "    \n",
    "    # Return early if chunked_data is empty\n",
    "    if not chunked_data:\n",
    "        print(\"No content identified in the document\")\n",
    "        return interesting_sections\n",
    "    \n",
    "    # Create block index for faster lookup - map page numbers to blocks on that page\n",
    "    page_to_blocks = defaultdict(list)\n",
    "    for block in text_blocks:\n",
    "        # Pre-normalize block text once\n",
    "        content = block.get(\"content\", \"\")\n",
    "        if content:\n",
    "            block[\"norm_text\"] = ' '.join(content.split())\n",
    "            page_to_blocks[block.get(\"page_num\", 0)].append(block)\n",
    "    \n",
    "    # Function to find matching block for a segment with improved fuzzy matching\n",
    "    def find_matching_block(segment, page_hint=None):\n",
    "        print(f\"Finding matching block for: {segment[:50]} ...\") \n",
    "        print(f\"Page hint: {page_hint}\")\n",
    "        norm_segment = ' '.join(segment.split())\n",
    "        if not norm_segment:\n",
    "            return None, False\n",
    "            \n",
    "        # First try exact page if provided\n",
    "        if page_hint is not None:\n",
    "            for block in page_to_blocks.get(page_hint, []):\n",
    "                if norm_segment in block[\"norm_text\"]:\n",
    "                    return block, True\n",
    "        \n",
    "        # Try exact matching across all pages\n",
    "        for page, page_blocks in page_to_blocks.items():\n",
    "            for block in page_blocks:\n",
    "                if norm_segment in block[\"norm_text\"]:\n",
    "                    return block, True\n",
    "        \n",
    "        # Try finding any substantial overlap\n",
    "        best_match = None\n",
    "        highest_similarity = 0.6  # Lowered threshold for better recall\n",
    "        best_overlap_len = 0\n",
    "        \n",
    "        # First prioritize matches by page if available\n",
    "        if page_hint is not None:\n",
    "            for block in page_to_blocks.get(page_hint, []):\n",
    "                # Try SequenceMatcher similarity\n",
    "                similarity = difflib.SequenceMatcher(None, norm_segment, block[\"norm_text\"]).ratio()\n",
    "                if similarity > highest_similarity:\n",
    "                    highest_similarity = similarity\n",
    "                    best_match = block\n",
    "                \n",
    "                # Try finding common substring\n",
    "                common_words = set(norm_segment.split()) & set(block[\"norm_text\"].split())\n",
    "                if len(common_words) > best_overlap_len:\n",
    "                    best_overlap_len = len(common_words)\n",
    "                    if not best_match:  # Only replace if we don't have a high similarity match\n",
    "                        best_match = block\n",
    "        \n",
    "        # If no good match on the hinted page, try all pages\n",
    "        if not best_match or highest_similarity < 0.7:\n",
    "            for page, page_blocks in page_to_blocks.items():\n",
    "                for block in page_blocks:\n",
    "                    similarity = difflib.SequenceMatcher(None, norm_segment, block[\"norm_text\"]).ratio()\n",
    "                    if similarity > highest_similarity:\n",
    "                        highest_similarity = similarity\n",
    "                        best_match = block\n",
    "                    \n",
    "                    # Try finding common substring as backup approach\n",
    "                    common_words = set(norm_segment.split()) & set(block[\"norm_text\"].split())\n",
    "                    overlap_ratio = len(common_words) / max(1, len(norm_segment.split()))\n",
    "                    if overlap_ratio > 0.5 and len(common_words) > best_overlap_len:\n",
    "                        best_overlap_len = len(common_words)\n",
    "                        if highest_similarity < 0.7:  # Only use word overlap if similarity isn't high\n",
    "                            best_match = block\n",
    "        \n",
    "        # If still no match, but we have a page hint, return any block from that page\n",
    "        if not best_match and page_hint is not None and page_hint in page_to_blocks and page_to_blocks[page_hint]:\n",
    "            return page_to_blocks[page_hint][0], False\n",
    "            \n",
    "        # If still no match, return any block from any page as last resort\n",
    "        if not best_match and page_to_blocks:\n",
    "            first_page = min(page_to_blocks.keys())\n",
    "            if page_to_blocks[first_page]:\n",
    "                return page_to_blocks[first_page][0], False\n",
    "        \n",
    "        return best_match, False\n",
    "    \n",
    "    # Function to extract page number from text with [Page X] annotation\n",
    "    def extract_page_number(text):\n",
    "        page_match = re.search(r'\\[Page (\\d+)\\]', text)\n",
    "        if page_match:\n",
    "            return int(page_match.group(1)) - 1  # Convert to 0-indexed\n",
    "        return None\n",
    "    \n",
    "    # Cache handling using structured JSON format\n",
    "    if os.path.exists(file_name):\n",
    "        try:\n",
    "            print(f\"Loading cached interesting points from {file_name}\")\n",
    "            # Try JSON format first (for files created by the optimized version)\n",
    "            try:\n",
    "                with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                    cached_data = json.load(f)\n",
    "                    \n",
    "                for segment_data in cached_data.get(\"segments\", []):\n",
    "                    segment = segment_data.get(\"text\", \"\")\n",
    "                    page_hint = segment_data.get(\"page\")\n",
    "                    \n",
    "                    if len(segment.split()) < threshold:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for page number in the text itself\n",
    "                    text_page = extract_page_number(segment)\n",
    "                    if text_page is not None:\n",
    "                        page_hint = text_page\n",
    "                        # Remove the page annotation if it exists\n",
    "                        segment_for_matching = re.sub(r'\\[Page \\d+\\]\\s*', '', segment).strip()\n",
    "                    else:\n",
    "                        segment_for_matching = segment\n",
    "                    \n",
    "                    matched_block, exact_match = find_matching_block(segment_for_matching, page_hint)\n",
    "                    if matched_block:\n",
    "                        # Add page annotation to the text\n",
    "                        page_num = matched_block.get(\"page_num\", 0)\n",
    "                        page_annotation = f\"[Page {page_num+1}]\"\n",
    "                        \n",
    "                        # Keep original text but ensure page annotation is correct\n",
    "                        if text_page is None:\n",
    "                            clean_segment = segment_for_matching\n",
    "                            display_text = f\"{page_annotation} {clean_segment}\"\n",
    "                        elif text_page != page_num:\n",
    "                            # Replace with correct page\n",
    "                            clean_segment = re.sub(r'\\[Page \\d+\\]\\s*', '', segment).strip()\n",
    "                            display_text = f\"{page_annotation} {clean_segment}\"\n",
    "                        else:\n",
    "                            display_text = segment  # Keep original if page is correct\n",
    "                        \n",
    "                        interesting_sections.append({\n",
    "                            \"page\": page_num,\n",
    "                            \"text\": display_text,\n",
    "                            \"category\": \"main\",\n",
    "                            \"bbox\": matched_block.get(\"occupy_space\"),\n",
    "                            \"page_text\": f\"Page {page_num+1}\"\n",
    "                        })\n",
    "                    else:\n",
    "                        # Still keep the segment but with a default page\n",
    "                        default_page = page_hint if page_hint is not None else 0\n",
    "                        page_annotation = f\"[Page {default_page+1}]\"\n",
    "                        \n",
    "                        # Handle page annotation in text\n",
    "                        if text_page is None:\n",
    "                            display_text = f\"{page_annotation} {segment}\"\n",
    "                        else:\n",
    "                            # Keep original annotation\n",
    "                            display_text = segment\n",
    "                        \n",
    "                        print(f\"Using default page for unmatched segment: {segment[:50]}...\")\n",
    "                        interesting_sections.append({\n",
    "                            \"page\": default_page,\n",
    "                            \"text\": display_text,\n",
    "                            \"category\": \"main\",\n",
    "                            \"bbox\": None,\n",
    "                            \"page_text\": f\"Page {default_page+1}\"\n",
    "                        })\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                # Fall back to the legacy format for backward compatibility\n",
    "                with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                cached_segments = re.findall(r'```segment(?:\\s*-\\s*(\\d+))?\\s*(.*?)```', content, re.DOTALL)\n",
    "                \n",
    "                for page_num_str, segment in cached_segments:\n",
    "                    segment = segment.strip()\n",
    "                    if len(segment.split()) < threshold:\n",
    "                        continue\n",
    "                    \n",
    "                    # Try to get page number from the text first\n",
    "                    text_page = extract_page_number(segment)\n",
    "                    if text_page is not None:\n",
    "                        page_hint = text_page\n",
    "                        # Remove the page annotation if it exists\n",
    "                        segment_for_matching = re.sub(r'\\[Page \\d+\\]\\s*', '', segment).strip()\n",
    "                    else:\n",
    "                        page_hint = int(page_num_str) - 1 if page_num_str else None\n",
    "                        segment_for_matching = segment\n",
    "                    \n",
    "                    matched_block, exact_match = find_matching_block(segment_for_matching, page_hint)\n",
    "                    \n",
    "                    if matched_block:\n",
    "                        # Add page annotation to the text\n",
    "                        page_num = matched_block.get(\"page_num\", 0)\n",
    "                        page_annotation = f\"[Page {page_num+1}]\"\n",
    "                        \n",
    "                        # Keep original text but ensure page annotation is correct\n",
    "                        if text_page is None:\n",
    "                            clean_segment = segment_for_matching\n",
    "                            display_text = f\"{page_annotation} {clean_segment}\"\n",
    "                        elif text_page != page_num:\n",
    "                            # Replace with correct page\n",
    "                            clean_segment = re.sub(r'\\[Page \\d+\\]\\s*', '', segment).strip()\n",
    "                            display_text = f\"{page_annotation} {clean_segment}\"\n",
    "                        else:\n",
    "                            display_text = segment  # Keep original if page is correct\n",
    "                        \n",
    "                        interesting_sections.append({\n",
    "                            \"page\": page_num,\n",
    "                            \"text\": display_text,\n",
    "                            \"category\": \"main\",\n",
    "                            \"bbox\": matched_block.get(\"occupy_space\"),\n",
    "                            \"page_text\": f\"Page {page_num+1}\"\n",
    "                        })\n",
    "                    else:\n",
    "                        # Still keep the segment but with a default page\n",
    "                        default_page = page_hint if page_hint is not None else 0\n",
    "                        page_annotation = f\"[Page {default_page+1}]\"\n",
    "                        \n",
    "                        if text_page is None:\n",
    "                            display_text = f\"{page_annotation} {segment}\"\n",
    "                        else:\n",
    "                            # Keep original annotation\n",
    "                            display_text = segment\n",
    "                        \n",
    "                        print(f\"Using default page for unmatched segment: {segment[:50]}...\")\n",
    "                        interesting_sections.append({\n",
    "                            \"page\": default_page,\n",
    "                            \"text\": display_text,\n",
    "                            \"category\": \"main\",\n",
    "                            \"bbox\": None,\n",
    "                            \"page_text\": f\"Page {default_page+1}\"\n",
    "                        })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading cached data: {e}. Generating new segments.\")\n",
    "            interesting_sections = []  # Reset and try generating new\n",
    "    \n",
    "    # Generate new segments if no cached results were loaded\n",
    "    if not interesting_sections:\n",
    "        try:\n",
    "            print(\"Generating new interesting points with LLM...\")\n",
    "            \n",
    "            prompt_template = PromptTemplate(\n",
    "                input_variables=[\"text\"],\n",
    "                template=\"\"\"\n",
    "                You are a reasoning summarizer.\n",
    "                Summarize the provided text and support your summary using different verbatim snippets from the original text.\n",
    "                Remember:\n",
    "                The reasoning section must ONLY contain verbatim text from the document.\n",
    "                Every sentence in the reasoning must be supporting sentences in the summary section.\n",
    "                Do not add any information that isn't directly from the document.\n",
    "                IMPORTANT: Do not remove page annotations like [Page X] from your snippets.\n",
    "                \n",
    "                Format each important snippet as:\n",
    "                ```segment - <page number>\n",
    "                <exact text from the document including the [Page X] annotation>\n",
    "                ```\n",
    "                \n",
    "                Below is text from the main content of a document in English:\n",
    "                {text}\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            # Concatenate text from all chunks\n",
    "            all_text = \"\"\n",
    "            for chunk in chunked_data:\n",
    "                # Add page annotation to make LLM include page numbers\n",
    "                page_num = chunk[\"page\"]\n",
    "                all_text += f\"[Page {page_num+1}] {chunk['text']}\\n\\n\"\n",
    "                \n",
    "            prompt = prompt_template.format(text=all_text)\n",
    "            \n",
    "            response = llm.invoke(prompt)\n",
    "            content = response.content\n",
    "            print(\"LLM response received\")\n",
    "            # Extract segments\n",
    "            segments = re.findall(r'```segment(?:\\s*-\\s*(\\d+))?\\s*(.*?)```', content, re.DOTALL)\n",
    "            print(f\"Extracted {len(segments)} segments from LLM response\")\n",
    "            \n",
    "            # Save in structured JSON format for better caching\n",
    "            cached_data = {\"segments\": []}\n",
    "            \n",
    "            for page_num_str, segment_text in segments:\n",
    "                segment = segment_text.strip()\n",
    "                if len(segment.split()) < threshold:\n",
    "                    continue\n",
    "                \n",
    "                # Try to extract page number from the text content first\n",
    "                text_page = extract_page_number(segment)\n",
    "                if text_page is not None:\n",
    "                    page_hint = text_page\n",
    "                    print(f\"Found page annotation in text: Page {page_hint+1}\")\n",
    "                    # Remove page annotation for matching but keep original text\n",
    "                    segment_for_matching = re.sub(r'\\[Page \\d+\\]\\s*', '', segment).strip()\n",
    "                else:\n",
    "                    page_hint = int(page_num_str) - 1 if page_num_str else None\n",
    "                    print(f\"Using segment marker page: Page {page_hint+1 if page_hint is not None else 'unknown'}\")\n",
    "                    segment_for_matching = segment\n",
    "                \n",
    "                matched_block, exact_match = find_matching_block(segment_for_matching, page_hint)\n",
    "                \n",
    "                # Always save the segment, even if no match is found\n",
    "                if matched_block:\n",
    "                    page_num = matched_block.get(\"page_num\", 0)\n",
    "                    page_annotation = f\"[Page {page_num+1}]\"\n",
    "                    \n",
    "                    # Store original segment in cache\n",
    "                    segment_data = {\n",
    "                        \"page\": page_num,\n",
    "                        \"text\": segment,\n",
    "                    }\n",
    "                    cached_data[\"segments\"].append(segment_data)\n",
    "                    \n",
    "                    # For display, ensure we have the correct page number\n",
    "                    if text_page is None:\n",
    "                        # No existing page annotation, add one\n",
    "                        display_text = f\"{page_annotation} {segment_for_matching}\"\n",
    "                    elif text_page != page_num:\n",
    "                        # Replace incorrect page annotation\n",
    "                        clean_segment = re.sub(r'\\[Page \\d+\\]\\s*', '', segment).strip()\n",
    "                        display_text = f\"{page_annotation} {clean_segment}\"\n",
    "                    else:\n",
    "                        display_text = segment  # Keep original if it had the correct page\n",
    "                    \n",
    "                    interesting_sections.append({\n",
    "                        \"page\": page_num,\n",
    "                        \"text\": display_text,\n",
    "                        \"category\": \"main\",\n",
    "                        \"bbox\": matched_block.get(\"occupy_space\"),\n",
    "                        \"page_text\": f\"Page {page_num+1}\"\n",
    "                    })\n",
    "                else:\n",
    "                    # Still save the segment with the hinted page or default to page 0\n",
    "                    default_page = page_hint if page_hint is not None else 0\n",
    "                    \n",
    "                    # Store the original segment in cache\n",
    "                    segment_data = {\n",
    "                        \"page\": default_page,\n",
    "                        \"text\": segment,\n",
    "                    }\n",
    "                    cached_data[\"segments\"].append(segment_data)\n",
    "                    \n",
    "                    # For display, use the page annotation from the text or add a default one\n",
    "                    if text_page is None:\n",
    "                        page_annotation = f\"[Page {default_page+1}]\"\n",
    "                        display_text = f\"{page_annotation} {segment}\"\n",
    "                    else:\n",
    "                        display_text = segment  # Keep original with its annotation\n",
    "                    \n",
    "                    print(f\"Using default page {default_page+1} for unmatched segment: {segment[:50]}...\")\n",
    "                    interesting_sections.append({\n",
    "                        \"page\": default_page,\n",
    "                        \"text\": display_text,\n",
    "                        \"category\": \"main\",\n",
    "                        \"bbox\": None,\n",
    "                        \"page_text\": f\"Page {default_page+1}\"\n",
    "                    })\n",
    "            \n",
    "            # Save results in structured format\n",
    "            with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cached_data, f, indent=2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during interesting point extraction: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"Found {len(interesting_sections)} interesting sections\")\n",
    "    return interesting_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_interesting_points(pdf_path, interesting_points, output_path):\n",
    "    \"\"\"Add highlights to the interesting points in the PDF with complete text block highlighting.\"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    \n",
    "    # Using cyan highlight color for main content\n",
    "    highlight_color = (0, 1, 1)  # RGB for cyan\n",
    "    fail_count = 0\n",
    "    success_count = 0\n",
    "    \n",
    "    for point in interesting_points:\n",
    "        page_num = point[\"page\"]\n",
    "        page = doc[page_num]\n",
    "        text = point[\"text\"]\n",
    "        if not text:\n",
    "            print(f\"Empty text for page {page_num}\")\n",
    "            continue\n",
    "            \n",
    "        # Extract content without page annotations for better matching\n",
    "        clean_text = re.sub(r'\\[Page \\d+\\]\\s*', '', text).strip()\n",
    "        if not clean_text:\n",
    "            continue\n",
    "            \n",
    "        # Try exact match first\n",
    "        text_instances = page.search_for(clean_text)\n",
    "        \n",
    "        found_matches = []\n",
    "        if not text_instances:\n",
    "            # Method 1: Try normalized text (remove extra whitespace)\n",
    "            normalized_text = ' '.join(clean_text.split())\n",
    "            text_instances = page.search_for(normalized_text)\n",
    "            \n",
    "            # Method 2: Try with key phrases and collect all matches to combine later\n",
    "            if len(normalized_text.split()) > 10:\n",
    "                # Extract significant phrases (5-8 words)\n",
    "                words = normalized_text.split()\n",
    "                for i in range(len(words) - 5):\n",
    "                    phrase = ' '.join(words[i:i+min(8, len(words)-i)])\n",
    "                    if len(phrase) > 15:  # Only phrases with enough content\n",
    "                        phrase_instances = page.search_for(phrase)\n",
    "                        if phrase_instances:\n",
    "                            found_matches.extend(phrase_instances)\n",
    "            \n",
    "            # Method 3: Use key sentences and collect all matches\n",
    "            if '.' in normalized_text:\n",
    "                sentences = [s.strip() for s in normalized_text.split('.') if len(s.strip()) > 15]\n",
    "                for sentence in sentences:\n",
    "                    sentence_instances = page.search_for(sentence)\n",
    "                    if sentence_instances:\n",
    "                        found_matches.extend(sentence_instances)\n",
    "        \n",
    "        # If we found partial matches, combine their bounding boxes to highlight full area\n",
    "        if found_matches and not text_instances:\n",
    "            if len(found_matches) >= 2:\n",
    "                # Sort rectangles by y-coordinate and then by x-coordinate\n",
    "                found_matches.sort(key=lambda r: (r[1], r[0]))  # Sort by top-y then left-x\n",
    "                \n",
    "                # Combine all matches into one larger rectangle\n",
    "                x0 = min(rect[0] for rect in found_matches)\n",
    "                y0 = min(rect[1] for rect in found_matches)\n",
    "                x1 = max(rect[2] for rect in found_matches)\n",
    "                y1 = max(rect[3] for rect in found_matches)\n",
    "                \n",
    "                # Create a single combined rectangle that covers all matches\n",
    "                combined_rect = (x0, y0, x1, y1)\n",
    "                text_instances = [combined_rect]\n",
    "            else:\n",
    "                text_instances = found_matches\n",
    "        \n",
    "        # Highlight found instances or use bbox as fallback\n",
    "        if text_instances:\n",
    "            for inst in text_instances:\n",
    "                highlight = page.add_highlight_annot(inst)\n",
    "                highlight.set_colors(stroke=highlight_color)\n",
    "                highlight.update()\n",
    "            success_count += 1\n",
    "        elif point.get(\"bbox_list\") and isinstance(point[\"bbox_list\"], list) and point[\"bbox_list\"]:\n",
    "            # If we have a list of bounding boxes, highlight all of them\n",
    "            for bbox in point[\"bbox_list\"]:\n",
    "                if bbox:  # Make sure bbox is not None\n",
    "                    r = page.add_highlight_annot(bbox)\n",
    "                    r.set_colors(stroke=highlight_color)\n",
    "                    r.update()\n",
    "            success_count += 1\n",
    "        elif point.get(\"bbox\"):\n",
    "            # Single bbox case\n",
    "            r = page.add_highlight_annot(point[\"bbox\"])\n",
    "            r.set_colors(stroke=highlight_color)    \n",
    "            r.update()\n",
    "            success_count += 1\n",
    "        else:\n",
    "            # Last resort: try to find a text chunk on the page containing some keywords\n",
    "            if len(clean_text.split()) > 3:\n",
    "                key_words = [w for w in clean_text.split() if len(w) > 5][:5]\n",
    "                for word in key_words:\n",
    "                    word_instances = page.search_for(word)\n",
    "                    if word_instances:\n",
    "                        for inst in word_instances:\n",
    "                            highlight = page.add_highlight_annot(inst)\n",
    "                            highlight.set_colors(stroke=(1, 0.5, 0))  # Orange for partial matches\n",
    "                            highlight.update()\n",
    "                        success_count += 1\n",
    "                        break\n",
    "            else:\n",
    "                fail_count += 1\n",
    "    \n",
    "    print(f\"Successfully highlighted {success_count} segments\")\n",
    "    print(f\"Failed to highlight {fail_count} segments\")\n",
    "    \n",
    "    # Save the highlighted PDF\n",
    "    doc.save(output_path)\n",
    "    doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_path:str)-> tuple[List[dict], str]:\n",
    "    \"\"\"PyMuPDF-based function to extract text with bounding boxes from a PDF file.\"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path, filetype=\"pdf\")\n",
    "        prev_block = None\n",
    "        all_blocks = []\n",
    "        for page_num, page in enumerate(doc):\n",
    "            words = page.get_text(\"words\")\n",
    "            page_block = []\n",
    "            # Take threshold based on page_width and page_height\n",
    "            WIDTH_threshold = (0.02 if page.rect.width > page.rect.height else 0.0092625) * page.rect.width\n",
    "            HEIGHT_threshold = (0.01 if page.rect.width > page.rect.height else 0.05) * page.rect.height\n",
    "            for curr_word in words:\n",
    "                # Each block is (x0, y0, x1, y1, text, block_no, block_type)\n",
    "                x0, y0, x1, y1, text, block_no, line_no, block_type = curr_word\n",
    "                text_height = y1 - y0\n",
    "                if not text.strip():\n",
    "                    continue\n",
    "                is_mergable = False\n",
    "                \n",
    "                \"\"\"\n",
    "                Check if the current block is close to the previous block.\n",
    "                The conditions are:\n",
    "                (1. The x-coordinates of the current block are within WIDTH_threshold of the previous block.\n",
    "                2. The y-coordinates of the current block are within HEIGHT_threshold of the previous block.\n",
    "                (3. The current block is not completely to the left of the previous block.\n",
    "                4. The y-coordinates of the current block are within 4 pixels of the previous block.\n",
    "                5. The current block is not completely to the right of the previous block.\n",
    "                \"\"\"\n",
    "                if prev_block and abs(text_height - prev_block[-1]) <= 0 and \\\n",
    "                    (\\\n",
    "                        (abs(x0 - prev_block[2]) <= WIDTH_threshold and (y0 -prev_block[3]) <= HEIGHT_threshold) \\\n",
    "                    ):\n",
    "                    prev_block[2] = max(prev_block[2], x1) \n",
    "                    prev_block[3] = max(prev_block[3], y1) \n",
    "                    prev_block[4] += \" \" + text.strip()\n",
    "                    is_mergable = True\n",
    "\n",
    "                if is_mergable and page_block:\n",
    "                    page_block.pop()\n",
    "                    page_block.append(\n",
    "                        {\n",
    "                            \"page\": page_num,\n",
    "                            \"bbox\": (prev_block[0], prev_block[1], prev_block[2], prev_block[3]),\n",
    "                            \"text\": prev_block[4],\n",
    "                            \"block_no\": block_no,\n",
    "                            \"block_type\": block_type,\n",
    "                            \"text_height\": prev_block[-1]\n",
    "                        }\n",
    "                    )    \n",
    "                    prev_block = [prev_block[0], prev_block[1], prev_block[2], prev_block[3], prev_block[4], block_no, block_type, prev_block[-1]]\n",
    "                else:\n",
    "                    page_block.append({\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": (x0, y0, x1, y1),\n",
    "                        \"text\": text.strip(),\n",
    "                        \"block_no\": block_no,\n",
    "                        \"block_type\": block_type,\n",
    "                        \"text_height\": text_height\n",
    "                    })\n",
    "                    # Update the previous block\n",
    "                    prev_block = [x0, y0, x1, y1, text.strip(), block_no, block_type, text_height]\n",
    "            # merged_blocks = process_text_blocks(page_block, eps=25, min_samples=1)\n",
    "            all_blocks.extend(page_block)\n",
    "        doc.close()\n",
    "        # Remove empty blocks\n",
    "        if not all_blocks:\n",
    "            raise Exception(\"PyMuPDF Failed or No text found in the PDF.\")\n",
    "        return (all_blocks, 'pyMuPDF')\n",
    "    except Exception as e:\n",
    "        print(\"The Error is\", e.with_traceback())\n",
    "        # return extract_text_with_Mistral_OCR(pdf_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_blocks(blocks, parsing_method: str):\n",
    "    \"\"\"Classify text blocks into main content, footnotes, and extra information.\"\"\"\n",
    "    if not blocks:\n",
    "        print(\"No blocks to analyze\")\n",
    "        return None\n",
    "\n",
    "    classified_blocks = []\n",
    "\n",
    "    if parsing_method == 'Mistral OCR':\n",
    "        # Implement classification for Mistral OCR if needed\n",
    "        for block in blocks:\n",
    "            text = block[\"text\"].strip() if isinstance(block.get(\"text\"), str) else \"\"\n",
    "            \n",
    "            # A placeholder for footnote detection (to be refined)\n",
    "            is_footnote_marker = False  \n",
    "            classification = \"footnote\" if is_footnote_marker else \"main\"\n",
    "\n",
    "            block[\"category\"] = classification\n",
    "            classified_blocks.append(block)\n",
    "\n",
    "    elif parsing_method == 'pyMuPDF':\n",
    "        # First pass: Compute a ratio for each block to later determine the middle range\n",
    "        ratios = []\n",
    "        for block in blocks:\n",
    "            text = block[\"text\"].strip() if isinstance(block.get(\"text\"), str) else \"\"\n",
    "            bbox = block.get(\"bbox\", [0, 0, 0, 0])\n",
    "            text_height = block.get(\"text_height\", 0)\n",
    "            # Calculate area of the bounding box (space)\n",
    "            space = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
    "            text_count = len(text.split())\n",
    "            ratio = (text_count / space) if space > 0 else 0\n",
    "            ratios.append(ratio)\n",
    "\n",
    "        # Calculate the lower and upper quantiles for the middle 25% - adjust as needed.\n",
    "        # Here, we use 15% and 40% quantiles from your earlier example.\n",
    "        lower_q = pd.Series(ratios).quantile(0.25)\n",
    "        upper_q = pd.Series(ratios).quantile(0.75)\n",
    "\n",
    "        # Second pass: Classify each block using the ratio thresholds and footnote checks.\n",
    "        for idx, block in enumerate(blocks):\n",
    "            text = block[\"text\"].strip() if isinstance(block.get(\"text\"), str) else \"\"\n",
    "            bbox = block.get(\"bbox\", [0, 0, 0, 0])\n",
    "            space = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
    "            text_count = len(text.split())\n",
    "            ratio = (text_count / space) if space > 0 else 0\n",
    "\n",
    "            # Improved footnote detection\n",
    "            footnote_markers = [\"*\", \"†\", \"‡\", \"§\", \"¶\", \"⁂\", \"⁎\", \"⁑\", \"⁕\"]\n",
    "            is_footnote_marker = (\n",
    "                any(text.startswith(prefix) for prefix in footnote_markers) or\n",
    "                any(text.startswith(str(i)) for i in range(1, 10))\n",
    "            )\n",
    "\n",
    "            # Classification logic:\n",
    "            # - If it's a footnote marker, mark as \"footnote\".\n",
    "            # - Otherwise, if the block falls within the middle thresholds and has sufficient words, it is \"main\".\n",
    "            # - Else, it is \"non-main\"\n",
    "            if is_footnote_marker:\n",
    "                classification = \"footnote\"\n",
    "            elif lower_q <= ratio <= upper_q and text_count > 20 and text_height > 0:\n",
    "                classification = \"main\"\n",
    "            else:\n",
    "                classification = \"extra\"\n",
    "\n",
    "            block[\"category\"] = classification\n",
    "            classified_blocks.append(block)\n",
    "\n",
    "    return classified_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Example usage\n",
    "# input_pdf_path = r\"D:\\DATA300\\AudioBookSum\\pdf\\Gilman.pdf\"\n",
    "# doc = pymupdf.open(input_pdf_path)\n",
    "# text = \"\"\n",
    "# median_collection = []\n",
    "# content = []\n",
    "# i= 1\n",
    "# possible_water_mark = None\n",
    "# has_wm = False\n",
    "# for i in range(len(doc[:3])):\n",
    "#     extracted_pages = sorted(extract_text_with_ocr_approach(input_pdf_path, page_num =i, visual_proof= True), key = lambda x: [x[\"occupy_space\"].y0, x[\"occupy_space\"].x0])\n",
    "#     if possible_water_mark and extracted_pages[-1][\"content\"] == possible_water_mark:\n",
    "#         has_wm = True\n",
    "#     if not extracted_pages:\n",
    "#         continue\n",
    "#     possible_water_mark = extracted_pages[-1][\"content\"]\n",
    "# # print(has_wm)\n",
    "# for i in range(1,len(doc)):\n",
    "#     print(f\"Processing page {i+1}/{len(doc)}...\")\n",
    "#     extracted_pages = sorted(extract_text_with_ocr_approach(input_pdf_path, page_num =i), key = lambda x: [x[\"occupy_space\"].y0, x[\"occupy_space\"].x0])\n",
    "#     if has_wm:\n",
    "#         extracted_pages.pop()\n",
    "#     content.extend(extracted_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 263 text blocks\n",
      "Loading cached interesting points from D:\\DATA300\\AudioBookSum\\pdf\\Clopath_interesting_points.txt\n",
      "Generating new interesting points with LLM...\n",
      "LLM response received\n",
      "Extracted 7 segments from LLM response\n",
      "Found page annotation in text: Page 1\n",
      "Finding matching block for: - lf we study the word \"art\" and its etymology we  ...\n",
      "Page hint: 0\n",
      "Using default page 1 for unmatched segment: - [Page 1]\n",
      "lf we study the word \"art\" and its etym...\n",
      "Found page annotation in text: Page 2\n",
      "Finding matching block for: - We find that art means skill, but in the fine ar ...\n",
      "Page hint: 1\n",
      "Using default page 2 for unmatched segment: - [Page 2]\n",
      "We find that art means skill, but in th...\n",
      "Found page annotation in text: Page 2\n",
      "Finding matching block for: - The source of art is the perception of the beaut ...\n",
      "Page hint: 1\n",
      "Using default page 2 for unmatched segment: - [Page 2]\n",
      "The source of art is the perception of ...\n",
      "Found page annotation in text: Page 2\n",
      "Finding matching block for: - We know that it is not imitation, neither mere t ...\n",
      "Page hint: 1\n",
      "Using default page 2 for unmatched segment: - [Page 2]\n",
      "We know that it is not imitation, neith...\n",
      "Found page annotation in text: Page 4\n",
      "Finding matching block for: - The work of the artist, however, in which is see ...\n",
      "Page hint: 3\n",
      "Using default page 4 for unmatched segment: - [Page 4]\n",
      "The work of the artist, however, in whi...\n",
      "Found page annotation in text: Page 4\n",
      "Finding matching block for: - We see to-day a deplorable tendency to painting  ...\n",
      "Page hint: 3\n",
      "Using default page 4 for unmatched segment: - [Page 4]\n",
      "We see to-day a deplorable tendency to ...\n",
      "Found page annotation in text: Page 4\n",
      "Finding matching block for: - If we cannot afford portraits painted by artists ...\n",
      "Page hint: 3\n",
      "Using default page 4 for unmatched segment: - [Page 4]\n",
      "If we cannot afford portraits painted b...\n",
      "Found 7 interesting sections\n",
      "Identified 7 interesting points in main content\n",
      "Successfully highlighted 6 segments\n",
      "Failed to highlight 0 segments\n",
      "Created highlighted PDF: D:\\DATA300\\AudioBookSum\\pdf\\Clopath_highlighted.pdf\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "base_name = os.path.splitext(input_pdf_path)[0]\n",
    "output = f\"{base_name}_highlighted.pdf\"\n",
    "chunked_data = chunk_text(content)\n",
    "# Initialize LLM\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash', temperature=0.7)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini LLM: {e}\")\n",
    "    print(\"Make sure you have set GOOGLE_API_KEY in your environment or .env file\")\n",
    "    exit(1)\n",
    "words, extract_method = extract_text(input_pdf_path)\n",
    "# print(extract_method)\n",
    "\n",
    "print(f\"Extracted {len(words)} text blocks\")\n",
    "\n",
    "# Classify text blocks\n",
    "# classified_blocks = classify_text_blocks(words, extract_method)\n",
    "# print(\"Classified text blocks\")\n",
    "# # Identify interesting points (main content only)\n",
    "interesting_points = identify_interesting_points(chunked_data, llm, words, f\"{base_name}_interesting_points.txt\")  # Save to file\n",
    "# load from memory\n",
    "print(f\"Identified {len(interesting_points)} interesting points in main content\")\n",
    "\n",
    "# # Highlight interesting points in the PDF\n",
    "highlight_interesting_points(input_pdf_path, interesting_points, output)\n",
    "print(f\"Created highlighted PDF: {output}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
